name: "Target Profile Scraperüìå"

on:
  # Scheduled run every 58 minutes
  schedule:
    - cron: '*/100 * * * *'  # Every 100 minutes
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      max_profiles:
        description: 'Max Profiles (0 = unlimited)'
        required: false
        default: '0'
      
      batch_size:
        description: 'Batch Size'
        required: false
        default: '10'

concurrency:
  group: target-scraper-${{ github.ref }}
  cancel-in-progress: true

jobs:
  scrape-targets:
    runs-on: ubuntu-latest
    timeout-minutes: 100  # Must finish before next scheduled run
    
    steps:
      - name: üîÑ Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: üì¶ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: üåê Setup Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      - name: üîß Install ChromeDriver
        uses: nanasess/setup-chromedriver@v2
      
      - name: ‚úÖ Verify Setup
        run: |
          python --version
          pip list | grep -E "(selenium|gspread|google)"
          google-chrome --version
          chromedriver --version
      
      - name: üéØ Run Target Scraper
        env:
          # DamaDam Credentials
          DAMADAM_USERNAME: ${{ secrets.DAMADAM_USERNAME }}
          DAMADAM_PASSWORD: ${{ secrets.DAMADAM_PASSWORD }}
          DAMADAM_USERNAME_2: ${{ secrets.DAMADAM_USERNAME_2 }}
          DAMADAM_PASSWORD_2: ${{ secrets.DAMADAM_PASSWORD_2 }}
          
          # Google Sheets
          GOOGLE_SHEET_URL: ${{ secrets.GOOGLE_SHEET_URL }}
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
          
          # Scraper Settings
          MAX_PROFILES_PER_RUN: ${{ github.event.inputs.max_profiles || '0' }}
          BATCH_SIZE: ${{ github.event.inputs.batch_size || '10' }}
          MIN_DELAY: '0.4'
          MAX_DELAY: '0.6'
          PAGE_LOAD_TIMEOUT: '30'
          SHEET_WRITE_DELAY: '0.9'
        
        run: |
          echo "üéØ Starting Target Scraper..."
          echo "‚è∞ Run time: $(date)"
          echo "üìã Max profiles: $MAX_PROFILES_PER_RUN"
          python target_scraper.py
      
      - name: üìä Upload Logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: target-scraper-logs-${{ github.run_number }}
          path: |
            *.log
            damadam_cookies.pkl
          retention-days: 3
